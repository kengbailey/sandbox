{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyannotate/embedding \n",
    "https://huggingface.co/pyannote/embedding\n",
    "\n",
    "### panns_inference \n",
    "https://www.elastic.co/blog/searching-by-music-leveraging-vector-search-audio-information-retrieval\n",
    "\n",
    "https://github.com/qiuqiangkong/panns_inference\n",
    "\n",
    "### Resemblyzer\n",
    "https://github.com/resemble-ai/Resemblyzer\n",
    "\n",
    "### OpenL3\n",
    "https://github.com/marl/openl3?tab=readme-ov-file\n",
    "\n",
    "https://www.e2enetworks.com/blog/audio-driven-search-leveraging-vector-databases-for-audio-information-retrieval\n",
    "\n",
    "### Towhee\n",
    "https://towhee.io/audio-embedding/vggish\n",
    "\n",
    "https://github.com/towhee-io/examples/blob/main/audio/audio_classification/music_genre_classification.ipynb\n",
    "\n",
    "https://towhee.io/tasks/detail/operator?field_name=Audio&task_name=Audio-Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyAnnotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyannote.audio import Inference\n",
    "from pyannote.audio import Model\n",
    "import torch\n",
    "\n",
    "model = Model.from_pretrained(\"pyannote/embedding\", \n",
    "                              use_auth_token=\"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\")\n",
    "inference = Inference(model, window=\"whole\n",
    "inference.to(torch.device(\"cuda\")) # optional\n",
    "                      \n",
    "# Whole          \n",
    "embedding1 = inference(\"BabyElephantWalk60.wav\") # 512 dimensions\n",
    "\n",
    "## Excerpt\n",
    "# excerpt = Segment(13.37, 19.81)\n",
    "# embedding = inference.crop(\"audio.wav\", excerpt)\n",
    "# `embedding` is (1 x D) numpy array extracted from the file excerpt.\n",
    "\n",
    "## Sliding Window\n",
    "# inference = Inference(model, window=\"sliding\",\n",
    "#                       duration=3.0, step=1.0)\n",
    "# embeddings = inference(\"audio.wav\")\n",
    "# sliding window, i.e. from [i * step, i * step + duration]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# embedding1\n",
    "print(len(embedding1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import cdist\n",
    "# distance = cdist(embedding1, embedding2, metric=\"cosine\")[0,0]\n",
    "# `distance` is a `float` describing how dissimilar speakers 1 and 2 are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# panns_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install panns-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: /home/jovyan/panns_data/Cnn14_mAP=0.431.pth\n",
      "GPU number: 1\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import panns_inference\n",
    "from panns_inference import AudioTagging, SoundEventDetection, labels\n",
    "\n",
    "model = AudioTagging(checkpoint_path=None, device='cuda')\n",
    "a, _ = librosa.load(\"BabyElephantWalk60.wav\", sr=44100)\n",
    "\n",
    "# Reshape the audio time series to have an extra dimension, which is required by the model's inference function.\n",
    "query_audio = a[None, :]\n",
    "\n",
    "# Perform inference on the reshaped audio using the model. This returns an embedding of the audio. \n",
    "_, emb = model.inference(query_audio) # 2048 dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "[[0.         0.         0.         ... 0.26346177 0.8452306  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(len(emb[0]))\n",
    "print(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resemblyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install Resemblyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cuda in 0.02 seconds.\n",
      "256\n",
      "[0.006 0.009 0.006 0.    0.048 0.002 0.023 0.038 0.013 0.103 0.145 0.174\n",
      " 0.052 0.017 0.048 0.012 0.11  0.017 0.03  0.003 0.004 0.081 0.058 0.\n",
      " 0.001 0.087 0.001 0.112 0.029 0.008 0.101 0.047 0.02  0.002 0.018 0.062\n",
      " 0.071 0.042 0.031 0.006 0.    0.031 0.088 0.102 0.051 0.077 0.023 0.014\n",
      " 0.066 0.08  0.    0.036 0.003 0.    0.009 0.002 0.002 0.    0.016 0.052\n",
      " 0.022 0.001 0.006 0.096 0.07  0.028 0.124 0.049 0.005 0.009 0.044 0.058\n",
      " 0.024 0.044 0.034 0.032 0.    0.039 0.003 0.033 0.002 0.    0.14  0.078\n",
      " 0.011 0.007 0.018 0.096 0.005 0.139 0.034 0.006 0.011 0.013 0.131 0.\n",
      " 0.047 0.    0.    0.004 0.006 0.04  0.06  0.072 0.034 0.036 0.014 0.074\n",
      " 0.017 0.147 0.04  0.074 0.    0.08  0.004 0.    0.05  0.005 0.065 0.006\n",
      " 0.084 0.07  0.116 0.089 0.069 0.068 0.127 0.139 0.071 0.094 0.047 0.\n",
      " 0.028 0.1   0.056 0.057 0.031 0.03  0.13  0.03  0.001 0.008 0.019 0.039\n",
      " 0.052 0.008 0.1   0.002 0.132 0.127 0.044 0.088 0.004 0.018 0.026 0.14\n",
      " 0.002 0.008 0.117 0.074 0.062 0.007 0.114 0.053 0.121 0.045 0.072 0.043\n",
      " 0.062 0.059 0.007 0.014 0.    0.08  0.067 0.067 0.026 0.053 0.081 0.093\n",
      " 0.135 0.062 0.026 0.044 0.073 0.    0.015 0.08  0.07  0.049 0.04  0.038\n",
      " 0.004 0.104 0.    0.042 0.075 0.105 0.057 0.094 0.018 0.135 0.043 0.068\n",
      " 0.    0.024 0.016 0.047 0.04  0.012 0.041 0.    0.112 0.071 0.001 0.043\n",
      " 0.    0.034 0.108 0.047 0.101 0.058 0.01  0.    0.05  0.056 0.005 0.077\n",
      " 0.    0.028 0.012 0.    0.008 0.    0.06  0.12  0.138 0.109 0.118 0.06\n",
      " 0.096 0.054 0.011 0.087 0.003 0.024 0.111 0.025 0.172 0.017 0.004 0.072\n",
      " 0.035 0.025 0.042 0.   ]\n"
     ]
    }
   ],
   "source": [
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "fpath = Path(\"BabyElephantWalk60.wav\")\n",
    "wav = preprocess_wav(fpath)\n",
    "\n",
    "encoder = VoiceEncoder()\n",
    "embed = encoder.embed_utterance(wav)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(len(embed)) # 256 dimensions\n",
    "print(embed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towhee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install towhee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from towhee import pipe, ops\n",
    "\n",
    "p = (\n",
    "      pipe.input('path')\n",
    "          .map('path', 'frame', ops.audio_decode.ffmpeg())\n",
    "          .map('frame', 'vecs', ops.audio_embedding.vggish())\n",
    "          .output('vecs')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "62\n",
      "[[-0.27  -0.055 -0.137 ... -0.162  0.229  0.178]\n",
      " [-0.103 -0.073  0.483 ... -0.342 -0.149 -0.216]\n",
      " [ 0.014 -0.22   0.661 ... -0.405 -0.133 -0.235]\n",
      " ...\n",
      " [-0.73  -0.093  0.773 ... -0.863 -0.113 -0.031]\n",
      " [-0.436  0.073  0.486 ... -0.612 -0.114 -0.035]\n",
      " [-0.412  0.082  0.489 ... -0.582 -0.202 -0.118]]\n"
     ]
    }
   ],
   "source": [
    "embedd = p('BabyElephantWalk60.wav').get()[0] # 128 dimensions\n",
    "print(len(embedd[0])) # 128\n",
    "print(len(embedd)) # 62 \"slices\" ... 1 slice is about 1 second (0.9s)\n",
    "print(embedd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
